<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kubernetes - Production Deployments for Developers</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Recommendations for secure applications development with Rust">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox" class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded "><a href="01_Introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="expanded "><a href="02_Development.html"><strong aria-hidden="true">2.</strong> Setting up Dev Environment</a></li><li><ol class="section"><li class="expanded "><a href="02_01_Local_Registry.html"><strong aria-hidden="true">2.1.</strong> Setting up local registry</a></li></ol></li><li class="expanded "><a href="03_00_Scaffolding.html"><strong aria-hidden="true">3.</strong> Scaffolding Manifest Files</a></li><li><ol class="section"><li class="expanded "><a href="03_01_kubekutr_config.html"><strong aria-hidden="true">3.1.</strong> Understanding kubekutr config</a></li><li class="expanded "><a href="03_02_generate_base.html"><strong aria-hidden="true">3.2.</strong> Generating base manifests</a></li></ol></li><li class="expanded "><a href="04_00_Kustomize.html"><strong aria-hidden="true">4.</strong> Kustomize the Manifests</a></li><li><ol class="section"><li class="expanded "><a href="04_01_Kustomize_Overlays.html"><strong aria-hidden="true">4.1.</strong> Creating Overlays</a></li><li class="expanded "><a href="04_02_Kustomize_Patches.html"><strong aria-hidden="true">4.2.</strong> Applying patches</a></li></ol></li><li class="expanded "><a href="05_00_Deploying.html"><strong aria-hidden="true">5.</strong> Deploying our App</a></li><li><ol class="section"><li class="expanded "><a href="05_01_Deploying_Job.html"><strong aria-hidden="true">5.1.</strong> Creating a Job</a></li><li class="expanded "><a href="05_02_Deploying_Storage.html"><strong aria-hidden="true">5.2.</strong> Adding storage to DB</a></li><li class="expanded "><a href="05_03_Deploying_microk8s.html"><strong aria-hidden="true">5.3.</strong> Deploying on microk8s</a></li></ol></li><li class="expanded "><a href="06_00_Final_word.html"><strong aria-hidden="true">6.</strong> A final word</a></li><li><ol class="section"><li class="expanded "><a href="06_01_Final_additional.html"><strong aria-hidden="true">6.1.</strong> Additional Resources</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Kubernetes - Production Deployments for Developers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p><a href="https://www.rust-lang.org">Kubernetes</a> has been growing in popularity to orchestrate deployments using containers. While there are a lot of good tutorials and writing material available on how what is Kubernetes or how the internal work, this guide is primarily aimed towards developers who want to get their application up and running on a Kubernetes cluster and further extending the same approach to handle production deployments.</p>
<p>In this guide, we'll talk about how to deploy a real-world OSS application on Kubernetes including all the ancillary components (database, etc). We'll talk about how to organize the manifests, performing deployments to local cluster, using GitOps methodology, create resource manifests for multiple environments &amp; managing application configs and secrets. These tasks cover the majority of real-world use-cases that will be helpful once you decide to go in <em>production</em>.</p>
<p>In this guide, we'll talk about how to deploy a real-world OSS application <a href="https://listmonk.app/">listmonk</a> on Kubernetes which is an open-source + self-hosted mailing list manager to send campaigns/newsletters. While deploying the application on the K8s cluster, we will develop a basic foundation that would help you to get started with Kubernetes. Some of the broader topics we'll learn:</p>
<ul>
<li>Building containers and deploying in a local cluster</li>
<li>Connecting app with a database which is also deployed in K8s</li>
<li>Handling one-off tasks like DB migrations</li>
<li>Structuring manifests using <code>kustomize</code></li>
<li>Managing config and secrets for your applications</li>
</ul>
<h2><a class="header" href="#target-audience" id="target-audience">Target Audience</a></h2>
<p>The guide intends to demonstrate how to deploy an application to a Kubernetes cluster running <strong>locally</strong>. The idea behind doing this is to show an example that demonstrates a deployment pattern that can be extended to handle a production deployment as well. This will be most helpful for developers in organizations where the bridge between a <strong>traditional</strong> <em>Ops</em> team and a developer is slim or non-existent.</p>
<h3><a class="header" href="#what-this-guide-is-not" id="what-this-guide-is-not">What this guide is not</a></h3>
<p>It is not intended to be a tutorial or a course on how Kubernetes works. There is plenty of good writing material available for that already. This guide is not about Containers or Docker as well. The purpose is rather to guide the <strong>developers</strong> to migrate the older applications on a Kubernetes cluster (or deploy new apps) and gather them with all the tools or skills required to deploy/debug changes with confidence. This guide aims to make the developers self-sufficient when it comes to deployment to Kubernetes and let that task not be seen as something which a dedicated Ops team would do.</p>
<p>This guide covers my experience of how I do deployments at my organisation. You may find a lot of different workflows using <code>Helm</code> or other tools and you are encouraged to explore what works best for you. This guide in no way tries to <em>preach</em> any tool or a workflow, just demonstrates what works for me and potentially could work for a lot of projects as well.</p>
<h2><a class="header" href="#contributions" id="contributions">Contributions</a></h2>
<p>This guide is written in an open-collaborative form, via the GitHub platform <a href="https://github.com/mr-karan/k8s-deployment-book">k8s-deployment-book</a>. All contributions for future versions are most welcome, in the form of PRs and Issues.</p>
<h2><a class="header" href="#structure-of-the-guide" id="structure-of-the-guide">Structure of the Guide</a></h2>
<ul>
<li>We will first install all the tools required to set up a local environment which includes a local Kubernetes cluster as well.</li>
<li>Then we will further explore how to create resources manifest and organize them.</li>
<li>We will further look into how to deploy these changes to your Kubernetes clusters.</li>
<li>As we progress and you get a fair idea of how deployments are done locally, we will demonstrate how to deploy to a local K8s cluster.</li>
<li>At this point, we would have covered the deployment steps but there are some times when deployment doesn't go as planned so we will look at how to effectively debug applications using different <code>kubectl</code> commands.</li>
</ul>
<p>You can checkout the source code for all the manifests used in this guide <a href="https://github.com/mr-karan/listmonk-infra">here</a>.</p>
<h1><a class="header" href="#tools" id="tools">Tools</a></h1>
<p>You need the following tools for setting up the deployment workflow on Kubernetes <strong>locally</strong>.</p>
<h2><a class="header" href="#microk8s" id="microk8s">microk8s</a></h2>
<blockquote>
<p>The smallest, fastest, fully-conformant Kubernetes that track upstream releases and makes clustering trivial. MicroK8s is great for offline development, prototyping, and testing.</p>
</blockquote>
<p>We'll need a local instance of Kubernetes running so we can deploy the manifests and see if everything is running perfectly. There are a lot of options like <code>minikube</code>, <code>microk8s</code> <code>kind</code> etc. In this guide, we will choose <a href="https://microk8s.io/">microk8s</a> because it's quite easy to set up and get all components up and running without much pain.</p>
<p><code>microk8s</code> installs a lightweight Kubernetes cluster with bare minimum components required in the control plane. Additional <em>add-ons</em> can be configured with <code>microk8s enable &lt;addon-name&gt;</code>.</p>
<p><strong>NOTE</strong>: I tried other alternatives as well before <code>microk8s</code> and here are a few reasons to not go ahead with them:</p>
<ul>
<li><code>minikube</code> runs everything in a VM which is quite slow and resource-intensive.</li>
<li>A minor issue with <code>kind</code> is that if you reboot your system, the <code>docker</code> container in which the control plane runs is in <code>stopped</code> state. There's no <code>kind restart</code> as of <a href="https://github.com/kubernetes-sigs/kind/issues/148">yet</a>, so you'll have to re-deploy your app after recreating the cluster.</li>
</ul>
<p>It's no biggie if you prefer another <a href="https://www.cncf.io/certification/software-conformance/">CNCF compliant</a> K8s platform of your choice, the rest of the guide remains the same for you.</p>
<p><strong>Install Instructions</strong>: <a href="https://microk8s.io/docs/">microk8s.io/docs</a></p>
<p>Once you've installed <code>microk8s</code>, let's verify that our cluster is up.</p>
<pre><code class="language-shell">$ microk8s.kubectl get nodes
NAME   STATUS   ROLES    AGE     VERSION
work   Ready    &lt;none&gt;   6m22s   v1.17.2
</code></pre>
<h2><a class="header" href="#kubectl" id="kubectl">kubectl</a></h2>
<blockquote>
<p>Kubectl is a command-line tool for interacting with the Kubernetes API server and managing the cluster.</p>
</blockquote>
<p>You can read more about <code>kubectl</code> <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">here</a></p>
<h3><a class="header" href="#install-kubectl" id="install-kubectl">Install kubectl</a></h3>
<p>You can find instructions to install <code>kubectl</code> in your system <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">here</a>.</p>
<h3><a class="header" href="#configure-kubectl" id="configure-kubectl">Configure kubectl</a></h3>
<p><code>kubectl</code> looks for <code>KUBECONFIG</code> environment variable or <code>~/.kube/config</code> for path to <code>config file</code>. The config file consists of metadata about the cluster and the user. <code>kubectl</code> is a CLI wrapper for HTTP calls to the API server. As you guessed, the API has Authorization and Authentication and if you have multiple users in a cluster to manage you need <code>kubectl</code> to tell which user it's going to connect as. This piece of information is called a <strong>context</strong>. <code>context</code> holds the cluster name and user name. You can easily switch between <code>contexts</code> to log in as a different user or a different cluster altogether. Think of &quot;context&quot; as <em>profiles</em>. The context which is active at the moment is called <code>current-context</code>.</p>
<h4><a class="header" href="#power-tools-for-kubectl" id="power-tools-for-kubectl">Power tools for kubectl</a></h4>
<p><a href="https://github.com/ahmetb/kubectx">kubectx</a> is a nice utility to switch between clusters and namespaces.</p>
<h2><a class="header" href="#kubekutr" id="kubekutr">kubekutr</a></h2>
<blockquote>
<p>Cookie cutter templating tool for scaffolding K8s manifests</p>
</blockquote>
<p><strong>Disclaimer</strong>: This is a project which I developed after wrangling a lot of Kubernetes resource manifests by hand.</p>
<p>You can find more details about the project on <a href="https://github.com/mr-karan/kubekutr/">GitHub</a>.</p>
<h3><a class="header" href="#install-kubekutr" id="install-kubekutr">Install kubekutr</a></h3>
<p>You can download pre-compiled binaries for <code>kubekutr</code> from <a href="https://github.com/mr-karan/kubekutr/releases">GitHub</a>.
Grab the latest version from there and put it in your $PATH (e.g. to <code>/usr/local/bin/kubekutr</code>)</p>
<h2><a class="header" href="#kustomize" id="kustomize">kustomize</a></h2>
<blockquote>
<p>Customization of Kubernetes YAML configurations</p>
</blockquote>
<h3><a class="header" href="#install-kustomize" id="install-kustomize">Install kustomize</a></h3>
<p>You can find instructions to install <code>kubectl</code> in your system <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/INSTALL.md">here</a>.</p>
<h1><a class="header" href="#setting-up-a-local-registry" id="setting-up-a-local-registry">Setting up a local registry</a></h1>
<p>Let's pull the image of <code>listmonk</code> from <a href="https://hub.docker.com/r/listmonk/listmonk">DockerHub</a> and tag it locally. The reason to tag an image locally is that this guide is primarily focussed on developers who will be building their apps on the local system and testing the deployment workflow on local systems, so it doesn't make sense for us to use a remote image from Dockerhub as an example.</p>
<pre><code class="language-shell">docker pull listmonk/listmonk:v0.5.0-alpha
docker tag listmonk/listmonk:v0.5.0-alpha localhost:32000/listmonk:0.5
</code></pre>
<p>We should have an image <code>localhost:32000/listmonk:0.5</code> available locally. You can verify the same by:</p>
<pre><code class="language-shell">docker images --format &quot;{{.Repository}} : {{.Tag}}&quot;  | grep localhost:32000/listmonk
</code></pre>
<h2><a class="header" href="#local-registry-with-microk8s" id="local-registry-with-microk8s">Local Registry with microK8s</a></h2>
<p><code>microk8s</code> has an <code>addon</code> to enable a private Docker registry. The registry is exposed to your node on <code>localhost:32000</code>. You can read more about it <a href="https://microk8s.io/docs/registry-built-in">here</a>. This means that we can push our local images to this local registry. This comes super handy in case you want to test your local images.</p>
<p>To enable the local registry addon:</p>
<pre><code class="language-shell">microk8s enable registry
</code></pre>
<p>In case you are wondering why <code>microk8s</code> isn't available to find the images built locally, it is because <code>microk8s</code> runs it's own <code>containerd</code> daemon. <code>containerd</code> is the container runtime used to manage images in <code>microk8s</code>. Docker in your system also uses <code>containerd</code> but these two are 2 different services running. The images you tag and build locally, only your local <code>containerd</code> (the one which comes with Docker) knows about it. The <code>containerd</code> in <code>microk8s</code> has no information about this. That's why we use a local registry to push the images on a local registry that <code>contaienrd</code> of <code>microk8s</code> is aware of.</p>
<h2><a class="header" href="#push-the-image-to-the-local-registry" id="push-the-image-to-the-local-registry">Push the image to the local registry</a></h2>
<p>Now that our local Docker registry is running, we simply need to <code>push</code> our local tagged image to this registry.</p>
<pre><code class="language-shell">$ docker push localhost:32000/listmonk

The push refers to repository [localhost:32000/listmonk]
f2851807903a: Layer already exists
c58b6e921dd6: Layer already exists
e6e719f4def9: Layer already exists
5074c9cb3658: Layer already exists
03901b4a2ea8: Layer already exists
0.5: digest: sha256:b9f1c584e1f434eb9de92ec9d2e42da22fa41c831281d6f53888d735b2fb2bb1 size: 1365
</code></pre>
<h1><a class="header" href="#learning-the-basics" id="learning-the-basics">Learning the basics</a></h1>
<p>Kubernetes controllers are basically on an infinite reconciliation loop. Controllers in Kubernetes are control loops that watch the state of your cluster and if the real-world state drifts apart from the desired state, it &quot;reacts&quot; to those changes. Any controller in K8s tracks a particular resource spec, for example, the Deployment controller would track the Pods.</p>
<p><img src="./img/nutshell.png" alt="K8s in a nuteshell" /></p>
<p>The resource spec of an object in Kubernetes describes the desired state. The desired state has some metadata about the object, the <code>apiVersion</code> to use (specifying K8s API version), the name of the object, etc. Once you create this object, the controller manipulates the same <code>spec</code> to show information about the real world state as well under the field <code>status</code>. The controller would continuously monitor the real and desired state and trigger an event in the API server if it notices any drift.</p>
<p>Before we begin writing down the manifest files, we must understand why knowing about this behavior is important to us. Every single object you create in K8s is created with a manifest file. This manifest file declaratively sets the config values of the objects you create.</p>
<p>Now, the K8s API is quite verbose and not very straightforward for developers trying it out the first time. Also for any seasoned DevOps engineer, writing these manifest files becomes a monotonous and repetitive task. This served as an inspiration to build <code>kubekutr</code> which helps developers scaffold these manifests files from scratch. <code>kubekutr</code> takes minimum supported configuration values to create a <strong>base</strong> deployment. For complex configuration use cases, we use <code>kustomize</code> which modify our base manifests to create variants.</p>
<p>Let's proceed to writing our first <code>kubekutr</code> configuration file <a href="./03_01_kubekutr_config.html">here</a>.</p>
<h1><a class="header" href="#understanding-kubekutr-config" id="understanding-kubekutr-config">Understanding kubekutr config</a></h1>
<p>Let's create a folder <code>listmonk-infra</code> to store all our manifest configuration files. We will use this as a base folder for the rest of the guide unless specified otherwise.</p>
<pre><code class="language-shell">mkdir listmonk-infra &amp;&amp; cd listmonk-infra
</code></pre>
<p><code>kubekutr</code> needs to be configured with its own configuration file. To make things easier for newcomers, we have a default configuration template that can be edited according to your needs.</p>
<pre><code class="language-shell">kubekutr init --default
</code></pre>
<p>You should see <code>kubekutr.yml</code> generated by <code>kubekutr</code> in the previous step. Let's edit the file with the below configuration suited for <code>listmonk</code>:</p>
<pre><code class="language-yml">workloads:
  - name: listmonk # name of the project
    deployments:
      - name: app # name of the individual component
        replicas: 1
        labels:
          - name: 'app.kubernetes.io/component: app'
        containers:
          - name: app
            createService: true
            image: 'localhost:32000/listmonk:0.5'
            command: '[&quot;./listmonk&quot;]'
            args: '[&quot;--config&quot;, &quot;/etc/listmonk/config.toml&quot;]'
            envSecret: app-secrets
            ports:
            - name: app-port
              port: 9000
            cpuLimits: 800m
            memoryLimits: 500Mi
            cpuRequests: 400m
            memoryRequests: 250Mi
            readinessPort: 9000
            readinessPath: /
            livenessPort: 9000
            livenessPath: /
            volumeMounts:
              - name: config-dir
                mountPath: /etc/listmonk
        volumes:
          - name: config-dir
    services:
      - name: postgres
        type: ClusterIP
        headless: true
        ports:
          - name: db-port
            targetPort: db-port
            port: 5432
        labels:
          - name: 'app.kubernetes.io/component: svc-headless'
        selectors:
          - name: 'app.kubernetes.io/component: db'
    statefulsets:
      - name: db
        serviceName: postgres
        labels:
          - name: 'app.kubernetes.io/component: db'
        containers:
          - name: postgres
            image: 'postgres:12.2-alpine'
            ports:
            - name: db-port
              port: 5432
            envSecret: db-secrets
            volumeMounts:
              - name: postgres-storage
                mountPath: /var/lib/postgres
            cpuLimits: 500m
            memoryLimits: 800Mi
            cpuRequests: 250m
            memoryRequests: 400Mi
        volumes:
          - name: postgres-storage
</code></pre>
<h2><a class="header" href="#a-quick-overview" id="a-quick-overview">A quick overview</a></h2>
<p>Let's break down the giant config into pieces easy to understand.</p>
<h3><a class="header" href="#workloads" id="workloads">workloads</a></h3>
<p><code>Workload</code> represents a project/application. All of the different components of an application together form one workload.</p>
<p>So when we use:</p>
<pre><code class="language-yml">workloads:
  - name: listmonk # name of the project
  ...
</code></pre>
<p>We are naming our workload <code>listmonk</code> and going to add different components like the app container, the db container, etc to it.</p>
<h3><a class="header" href="#deployments" id="deployments">deployments</a></h3>
<p>Before we describe Deployments object, let's first understand how Kubernetes schedules the containers. Kubernetes control plane runs a component <a href="https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/#kube-scheduler">kube-scheduler</a>. The job of this scheduler is to talk to an agent running on different nodes (called <code>kubelet</code>) and figure out which node is the best to run your container depending on resource requirements.</p>
<p>A container in Kubernetes is basically wrapped around something called <strong>Pods</strong>. So Pods contain the information about network, storage resources, volume mounts, and the Docker image/arg etc. Kubernetes manages the pod directly, not the container so any changes that the controller makes are applied at the Pod level.</p>
<blockquote>
<p><strong>Sidecars</strong>: Running a single container per pod is the most common use case, but you often may find the usage of sidecars, which means running a lightweight container in addition to your application. Multiple containers together also form one Pod. The most common use cases of sidecar are for running logging/monitoring agents or running a lightweight web proxy etc.</p>
</blockquote>
<p><img src="./img/pod.png" alt="K8s pods" /></p>
<p>Pods are managed by <strong>Replica Set</strong>. Replica set controls the rollout/rollback of a group of pods. You can configure the rollout pattern by tweaking the <code>RollingUpdateStrategy</code> spec in <code>Deployment</code>.</p>
<p><img src="./img/replicaset.png" alt="K8s pods" /></p>
<p>The deployment contains the history of Replica sets at any point in time. The Deployment Controller changes the actual state to the desired state if there's any drift noticed.</p>
<p><img src="./img/deployment.png" alt="K8s pods" /></p>
<h3><a class="header" href="#services" id="services">services</a></h3>
<p><strong>Service</strong> object is used to expose your pod to other pods or to the world. There are multiple services types available for different use-cases about which you can read more <a href="https://kubernetes.io/docs/concepts/services-networking/service/">here</a>.</p>
<p><img src="./img/service.png" alt="K8s pods" /></p>
<p>In this guide, we will use the service of type <strong>ClusterIP</strong>. Each pod exposes an IP on the cluster and there are various ways to connect this IP. But what happens if you run multiple replicas of one app? You cannot expect the client to remember <em>n IPs</em> for <em>n replicas</em>. So, when you create a <code>Service</code> object of <code>ClusterIP</code> you get a virtual IP. This Service object backs all other IPs in the form of another object called <strong>Endpoints</strong>. The endpoint object is responsible for keeping a track of all Pod IPs that the Service object connects to. Kubernetes runs a component <code>kube-proxy</code> which resolves the virtual IP to the actual Pod IP present in <code>Endpoints</code>. This selection is <strong>not</strong> round-robin and completely random so it is very much possible that you see skewed traffic across pods.</p>
<p>Another crucial concept in service is <strong>Labels</strong> &amp; <strong>Selectors</strong>. While reading the above paragraph if you were thinking how does the Service object know which Pod IP to track, then the answer lies in using the field <code>.spec.selector</code> of the Service object.</p>
<p>To understand this easily, let's imagine our Deployment spec looks like:</p>
<pre><code class="language-yml">name: listmonk
replicas: 1
...
labels:
app: listmonk
tier: web
...
image: listmonk:latest
</code></pre>
<p>Now, to target the Pod with <strong>labels</strong> <code>app:listmonk</code> and <code>tier:web</code> we need to create a <code>Service</code> object which matches the <code>.spec.selector</code>:</p>
<pre><code class="language-yml">selector:
    app: listmonk
    tier: web
</code></pre>
<p>So, this is how using Labels and matching the equivalent values with Selectors we can target the Pods from a Service object.</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/b964c59cdc1979dd4e1904c25f43745564ef6bee/f3351/docs/tutorials/kubernetes-basics/public/images/module_04_labels.svg" alt="Service-K8s" /></p>
<p>Now that we have covered the broader concepts, let's start building the base manifest using <code>kubekutr</code> <a href="./03_02_generate_base.html">here</a>.</p>
<h1><a class="header" href="#generating-base-with-kubekutr" id="generating-base-with-kubekutr">Generating base with kubekutr</a></h1>
<pre><code class="language-shell"># cwd: listmonk-infra
$ kubekutr -c kubekutr.yml scaffold -o .

DEBU[2020-03-27T16:30:43+05:30] verbose logging enabled
INFO[2020-03-27T16:30:43+05:30] Starting kubekutr...
2020/03/27 16:30:43 reading config: kubekutr.yml
</code></pre>
<p>This generates a <code>base</code> folder in your current working directory.</p>
<pre><code class="language-shell">$ tree .
.
├── base
│   └── listmonk
│       ├── app-deployment.yml
│       ├── app-service.yml
│       ├── db-statefulset.yml
│       └── postgres-service.yml
└── kubekutr.yml

2 directories, 5 files
</code></pre>
<p>Inside <code>base</code> a folder for each <strong>workspace</strong> is created. As mentioned earlier, <code>workspace</code> contains all the components of an application. The deployments and service manifests are all present here.</p>
<blockquote>
<p><strong>NOTE</strong>: Remember to not edit these files by hand, as the next time you run <code>scaffold</code> on kubekutr, the changes will be overwritten.</p>
</blockquote>
<p>Now we have the base manifests ready, but we still need to bring <code>kustomize</code> in our toolchain to make use of this. So let's learn more about Kustomize in the next <a href="./04_Kustomize.html">section</a>.</p>
<h1><a class="header" href="#kustomize-the-manifests" id="kustomize-the-manifests">Kustomize the Manifests</a></h1>
<p>So far we have learned how to use <code>kubekutr</code> to write our base manifest files. We will need <code>kustomize</code> to <em>customize</em> the base YAML files generated by <code>kubekutr</code> in a template-free format.</p>
<h2><a class="header" href="#why-kustomize" id="why-kustomize">Why Kustomize</a></h2>
<p>In the previous section, we explored <code>kubekutr</code> to create manifests from scratch. However, when we do deployments across multiple environments there are cases where some things need to be changed based on the environment. For eg, in a local K8s cluster, since there's no cloud provisioner to create a <code>LoadBalancer</code> service, we need to change it to a <code>ClusterIP</code> or <code>NodePort</code>. Or if you've configured to run <code>10 replica</code>s of your pod to serve high traffic with higher resource limits, but you need to change these variables in a local setup.</p>
<p><img src="./img/kustomize.png" alt="kustomize" /></p>
<p><strong>kustomize</strong> helps us with that in a template-free manner. There are no ugly <em>if-else</em> in our templates (conditionals in templates are <em>bad</em>) which also makes the output of these templates as K8s <em>native</em>. <code>kustomize</code> takes the approach where we basically combine the <strong>base</strong> and apply <strong>patch</strong> to create a <strong>variant</strong>.</p>
<p>For eg, we created <code>base</code> using <code>kubekutr</code> which provides a starting point for any further configurations. Then based on the environment (<code>dev/staging/prod</code>) we create <strong>overalys</strong>. In these overalys, we again specify the things to be changed referencing a base called <strong>patches</strong>. The changes are applied on top of the base and the resultant output is called a <strong>variant</strong>. So if we create 3 overlays referencing the same base we get 3 variants to deploy based on the environment. This approach helps us to keep the base as clean as possible and only fiddle with the values that change across environments.</p>
<p>Let's proceed to write our first <code>kustomization.yml</code> file!</p>
<h2><a class="header" href="#create-kustomizationyml-file" id="create-kustomizationyml-file">Create kustomization.yml file</a></h2>
<p>Create a file <code>kustomization.yml</code> which represents the config for <code>kustomize</code> to build resources etc.</p>
<pre><code class="language-shell"># vim base/kustomization.yml
resources:
- listmonk/app-deployment.yml
- listmonk/app-service.yml
- listmonk/db-statefulset.yml
- listmonk/postgres-service.yml
</code></pre>
<p>First, we'll collect the resources used to build our base. These files are generated by <code>kubekutr</code> and we are just aggregating the resources so that when <code>kustomize build</code> runs, it parses all of these resources.</p>
<blockquote>
<p><strong>Note</strong>: As mentioned earlier, <code>kustomize</code> targets the root of the directory where <code>kustomization.yaml</code> is present.</p>
</blockquote>
<p>You can see the manifest generated with:</p>
<pre><code class="language-shell">kustomize build base
</code></pre>
<p>Now that we have built our <em>inventory</em> for the resources required to deploy <em>listmonk</em>, let's proceed to create an <strong>overlay</strong> for our base deployment <a href="./04_01_Kustomize_Overlays.html">here</a>.</p>
<h1><a class="header" href="#creating-overlays" id="creating-overlays">Creating Overlays</a></h1>
<p>Let's create a folder for an <strong>overlay</strong>. In this guide, we will follow the naming convention of naming the overlays folder based on the environment name. For eg, if we need an overlay for <code>local</code> env, we will make a directory <code>local</code> with <code>kustomization.yml</code> present at the root of this folder.</p>
<pre><code class="language-shell">mkdir -p overlays/local
</code></pre>
<p>As mentioned earlier, <code>kustomization.yml</code> should be present in the <strong>root</strong> of the <code>overlay</code> folder, so let's create that:</p>
<pre><code class="language-yml"># vim overlays/local/kustomization.yml
resources:
- ../../base

namespace: listmonk-local

# Common Prefix to be applied to all resources
namePrefix: listmonk-
nameSuffix: -local
# Common Labels applied to all resources
commonLabels:
  app.kubernetes.io/managed-by: team-listmonk

configMapGenerator:
  - name: app-config
    files:
      - config.toml=configs/config.toml

secretGenerator:
  - name: app-secrets
    literals:
      - LISTMONK_db__host=listmonk-postgres-local
  - name: db-secrets
    literals:
      - POSTGRES_PASSWORD=listmonk
      - POSTGRES_USER=listmonk
      - POSTGRES_DB=listmonk
</code></pre>
<p>In the above configuration we make use of <code>kustomize</code> field spec to apply certain changes:</p>
<ul>
<li><strong>resources</strong>: This references the <code>base</code> we created in the previous step. Without a <code>base</code> an overlay is useless since it has nothing to target.</li>
<li><strong>namespace</strong>: Name of the K8s <em>namespace</em>, where all resources will be deployed.</li>
<li><strong>namePrefix</strong>: Prepends <code>listmonk-</code> to each resource name.</li>
<li><strong>nameSuffix</strong>: Appends <code>-local</code> to each resource name.</li>
<li><strong>commonLabels</strong>: Applies a set of same labels to each resource.</li>
<li><strong>configMapGenerator</strong>: List of <code>configmaps</code> to be generated. Here, we use a file generator to create the ConfigMap object for us.</li>
<li><strong>secretGenerator</strong>: List of <code>secrets</code> to be generated. Here, we rely on environment variables to create a secret.</li>
</ul>
<p>You can read more about the <code>kustomization.yml</code> field spec in the official <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md">docs</a>.</p>
<h2><a class="header" href="#generating-configmap" id="generating-configmap">Generating ConfigMap</a></h2>
<p><code>kustomize</code> lets us build a <strong>ConfigMap</strong> object from raw files, literal strings or environment variables. Since <code>listmonk</code> expects a <code>config.toml</code> configuration to start the application, we will use the <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/plugins/builtins.md#field-name-configMapGenerator">configMapGenerator</a> for the same.</p>
<pre><code class="language-yml"># vim overlays/local/kustomization.yml
configMapGenerator:
  - name: app-config
    files:
      - config.toml=configs/config.toml
</code></pre>
<p>In the field <code>files</code>, the <code>key</code> (<em>config.toml</em>) is the key used in <code>ConfigMap</code> while mounting the volume. For eg, in our <code>kubekutr.yml</code> we mentioned the following information for the <em>Deployment</em> spec:</p>
<pre><code class="language-yml">volumeMounts:
  - name: config-dir
    mountPath: /etc/listmonk
</code></pre>
<p>This basically mounts all the keys present in <code>config-dir</code> <em>ConfigMap</em> object inside <code>/etc/listmonk</code> directory. In our case, we only have one file <code>config.toml</code> though.</p>
<p>We will need to place the sample config file for <code>listmonk</code> so that <code>kustomize</code> can create the object:</p>
<pre><code class="language-shell">mkdir -p overlays/local/configs
</code></pre>
<pre><code class="language-toml"># vim overlays/local/configs/config.toml
[app]
# Interface and port where the app will run its webserver.
address = &quot;0.0.0.0:9000&quot;

# Public root URL of the listmonk installation that'll be used
# in the messages for linking to images, unsubscribe page, etc.
root = &quot;https://listmonk.mysite.com&quot;

# (Optional) full URL to the static logo to be displayed on
# user facing view such as the unsubscription page.
# eg: https://mysite.com/images/logo.svg
logo_url = &quot;https://listmonk.mysite.com/public/static/logo.png&quot;

# (Optional) full URL to the static favicon to be displayed on
# user facing view such as the unsubscription page.
# eg: https://mysite.com/images/favicon.png
favicon_url = &quot;https://listmonk.mysite.com/public/static/favicon.png&quot;

# The default 'from' e-mail for outgoing e-mail campaigns.
from_email = &quot;listmonk &lt;from@mail.com&gt;&quot;

# List of e-mail addresses to which admin notifications such as
# import updates, campaign completion, failure etc. should be sent.
# To disable notifications, set an empty list, eg: notify_emails = []
notify_emails = [&quot;admin1@mysite.com&quot;, &quot;admin2@mysite.com&quot;]

# Maximum concurrent workers that will attempt to send messages
# simultaneously. This should depend on the number of CPUs the
# machine has and also the number of simultaneous e-mails the
# mail server will
concurrency = 100

# The number of errors (eg: SMTP timeouts while e-mailing) a running
# campaign should tolerate before it is paused for manual
# investigation or intervention. Set to 0 to never pause.
max_send_errors = 1000


[privacy]
# Allow subscribers to unsubscribe from all mailing lists and mark themselves
# as blacklisted?
allow_blacklist = false

# Allow subscribers to export data recorded on them?
allow_export = false

# Items to include in the data export.
# profile            Subscriber's profile including custom attributes
# subscriptions      Subscriber's subscription lists (private list names are masked)
# campaign_views     Campaigns the subscriber has viewed and the view counts
# link_clicks        Links that the subscriber has clicked and the click counts
exportable = [&quot;profile&quot;, &quot;subscriptions&quot;, &quot;campaign_views&quot;, &quot;link_clicks&quot;]

# Allow subscribers to delete themselves from the database?
# This deletes the subscriber and all their subscriptions.
# Their association to campaign views and link clicks are also
# removed while views and click counts remain (with no subscriber
# associated to them) so that stats and analytics aren't affected.
allow_wipe = false


# Database.
[db]
host = &quot;demo-db&quot;
port = 5432
user = &quot;listmonk&quot;
password = &quot;listmonk&quot;
database = &quot;listmonk&quot;
ssl_mode = &quot;disable&quot;

# Maximum active and idle connections to pool.
max_open = 50
max_idle = 10

# SMTP servers.
[smtp]
    [smtp.my0]
        enabled = true
        host = &quot;my.smtp.server&quot;
        port = &quot;25&quot;

        # cram | plain | empty for no auth
        auth_protocol = &quot;cram&quot;
        username = &quot;xxxxx&quot;
        password = &quot;&quot;

        # Optional. Some SMTP servers require a FQDN in the hostname.
        # By default, HELLOs go with &quot;localhost&quot;. Set this if a custom
        # hostname should be used.
        hello_hostname = &quot;&quot;

        # Maximum time (milliseconds) to wait per e-mail push.
        send_timeout = 5000

        # Maximum concurrent connections to the SMTP server.
        max_conns = 10

    [smtp.postal]
        enabled = false
        host = &quot;my.smtp.server2&quot;
        port = &quot;25&quot;

        # cram or plain.
        auth_protocol = &quot;plain&quot;
        username = &quot;xxxxx&quot;
        password = &quot;&quot;

        # Maximum time (milliseconds) to wait per e-mail push.
        send_timeout = 5000

        # Maximum concurrent connections to the SMTP server.
        max_conns = 10

# Upload settings
[upload]
# Provider which will be used to host uploaded media. Bundled providers are &quot;filesystem&quot; and &quot;s3&quot;.
provider = &quot;filesystem&quot;

# S3 Provider settings
[upload.s3]
# (Optional). AWS Access Key and Secret Key for the user to access the bucket. Leaving it empty would default to use
# instance IAM role.
aws_access_key_id = &quot;&quot;
aws_secret_access_key = &quot;&quot;
# AWS Region where S3 bucket is hosted.
aws_default_region=&quot;ap-south-1&quot;
# Specify bucket name.
bucket=&quot;&quot;
# Path where the files will be stored inside bucket. Empty value (&quot;&quot;) means the root of bucket.
bucket_path=&quot;&quot;
# Bucket type can be &quot;private&quot; or &quot;public&quot;.
bucket_type=&quot;public&quot;
# (Optional) Specify TTL (in seconds) for the generated presigned URL. Expiry value is used only if the bucket is private.
expiry=&quot;86400&quot;

# Filesystem provider settings
[upload.filesystem]
# Path to the uploads directory where media will be uploaded. Leaving it empty (&quot;&quot;) means current working directory.
upload_path=&quot;&quot;
# Upload URI that's visible to the outside world. The media uploaded to upload_path will be made available publicly
# under this URI, for instance, list.yoursite.com/uploads.
upload_uri = &quot;/uploads&quot;
</code></pre>
<blockquote>
<p><strong>Note</strong>:  <code>kustomize</code> appends a random hash string to the name of <code>ConfigMap</code> or <code>Secret</code> object if their content changes, thus triggering a new deployment automatically. A feature that K8s still doesn't have and people resort to hacks like these even in Helm :). Be careful about garbage collection with this as old ConfigMaps and Secret objects need to purged to not take up <em>ever increasing</em> space in your <code>etcd</code> cluster.</p>
</blockquote>
<h2><a class="header" href="#building-the-overlay" id="building-the-overlay">Building the overlay</a></h2>
<p>At this point, we can test that our overlay works correctly by <strong>building</strong> it:</p>
<pre><code class="language-shell">kustomize build overlays/local
</code></pre>
<p>You should see an output of all manifest files aggregated with our <em>customisations</em> applied on the base. Let's proceed on how to create a patch <a href="./04_02_Kustomize_Patches.html">here</a></p>
<h1><a class="header" href="#applying-patches" id="applying-patches">Applying patches</a></h1>
<p>A very important and crucial feature of <code>kustomize</code> is its ability to create different <em>variants</em> of our deployment without templating using <strong>patches</strong>.</p>
<p><a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/plugins/builtins.md#patchesstrategicmerge">patchesStrategicMerge</a> applies patches to list of resources which are matched by some unique identifier (<code>Group/Version/Kind + Name/Namespace</code>). The patch contains only a sparse resource spec, omitting the fields that are already defined in the base.</p>
<p>For eg, to change a replica count in Deployment, we will omit all other fields and only have this field listed in the resource spec.</p>
<pre><code class="language-yml"># vim example-increase-replica.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    tier: api
spec:
  replicas: 1
</code></pre>
<p>This <code>patch</code> targets an object of <em>kind</em> <code>Deployment</code> with <em>apiVersion</em> <code>apps/v1</code> and <em>name</em> as <code>app</code> with labels <code>tier:api</code> set. As you can see we only mention the fields that we have to replace which is <code>.spec.replicas</code>. <code>kustomize</code> will see the patch and only merge the <code>replicas</code> YAML key with the old manifest. So, in effect only <code>replicas</code> key is changed in the manifest, everything else remains the same.</p>
<p>Now that the basics of how a <code>patch</code> works are covered, let's take a look at how we'll use <code>patchesStrategicMerge</code> to fill some gaps by <code>kubekutr</code>.</p>
<p>Going back to <code>kubekutr.yml</code>, we had mentioned the volume mount in the <code>deployment</code> field. <code>listmonk</code> requires a <code>config.toml</code> to source config variables for the app to run. Since <code>volume</code> can be of <a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">multiple types</a> and by design <code>kubekutr</code> sticks to a very generic and base config format, specifying the type of volume isn't possible in <code>kubekutr</code> as of yet.</p>
<p>No worries, we will use <code>kustomize</code> to add any such missing fields.</p>
<pre><code class="language-shell">mkdir -p base/patches
</code></pre>
<pre><code class="language-yml"># vim base/patches/add-config-volume.yml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app # Targets the deployment with `name:app`
spec:
  template:
    spec:
      volumes:
        - name: config-dir
          configMap:
            name: app-config
</code></pre>
<p>Let's add this patch to our <em>inventory</em> defined in <code>base/kustomization.yml</code>:</p>
<pre><code class="language-yml"># vim base/kustomization.yml
patchesStrategicMerge:
  - patches/add-config-volume.yml
</code></pre>
<p>This will tell <code>kustomize</code> to look for a patch present in <code>patches/</code> directory and apply the patch.</p>
<p>This was all about <strong>patches</strong> that you needed to know. There are some other strategies for patches to apply in <code>kustomize</code> like <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md#patchesjson6902">patchesJson6902</a> and <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/fields.md#patches">patches</a>. You can explore them on your own as it is out of scope for this guide.</p>
<h2><a class="header" href="#verifying-the-patch" id="verifying-the-patch">Verifying the patch</a></h2>
<p>Now we can test that our overlay along with patches works correctly by <strong>building</strong> it:</p>
<pre><code class="language-shell">kustomize build overlays/local
</code></pre>
<p>On inspecting the <code>Deployment</code> spec from the output, we can see the volume configured correctly:</p>
<pre><code class="language-yml">        volumeMounts:
        - mountPath: /etc/listmonk
          name: config-dir
      volumes:
      - configMap:
          name: listmonk-app-config-local-67m56g98mm
        name: config-dir
</code></pre>
<p>An interesting thing to note here is that <code>kustomize</code> applies a random hash to the <code>ConfigMap</code> object name as we can see in the above example (<code>listmonk-app-config-local-67m56g98mm</code>). You can see that the <code>ConfigMap</code> object is created with the same name.</p>
<p>If you're wondering where this is useful, imagine a use-case where let's say the ConfigMap name is <code>listmonk-app-config</code> and you change the ConfigMap contents. You'd imagine the app to be restarted for any config changes, but the Deployment controller doesn't track the ConfigMap updates. So since the name is the same in the new ConfigMap (<code>listmonk-app-config</code>), just that the contents have changed, the new Deployment is not rolled out.</p>
<p><code>kustomize</code> appends a random hash to the ConfigMap name and whenever the contents of the ConfigMap change, the hash is also changed. Since the name of <code>ConfigMap</code> object itself is changed in the Deployment spec, the controller notices the change and schedules a rollout of the new pods with the updated config.</p>
<p>This is just one of the many benefits <code>kustomize</code> provides over mangling resources by hand. I hope by now you're seeing the benefits of using <code>kustomize</code> and wanting to start using this in your toolchain.</p>
<h1><a class="header" href="#deploying-our-app" id="deploying-our-app">Deploying our App</a></h1>
<h2><a class="header" href="#a-quick-recap" id="a-quick-recap">A quick recap</a></h2>
<p>Let's take a moment to recap the things we have covered so far.</p>
<ul>
<li>Created a base manifest using <code>kubekutr</code>.</li>
<li>Applied patches to the base for configuring the volume mount.</li>
<li>Created overlays for <code>local</code> environment with <code>kustomize</code>.</li>
<li>Used <code>configMapGenerator</code> and <code>secretGenerator</code> to create these native objects from raw resources.</li>
</ul>
<p>We are pretty close to a full-fledged deployment of Listmonk. A few more things that we need to cover:</p>
<ul>
<li><a href="./05_01_Deploying_Job.html">Creating a Job</a>: <code>Job</code> object helps us to run one-off tasks like DB Migrations, pulling assets from S3, etc.</li>
<li><a href="./05_02_Deploying_Storage.html">Adding Persistent storage</a>: <code>Persistent Volume</code> helps us back the storage to a Pod which is necessary for stateful workloads like Databases etc.</li>
<li><a href="./05_03_Deploying_microk8s.html">Running on microK8s</a>: We will aggregate all the resources and apply the changes to the cluster.</li>
</ul>
<h1><a class="header" href="#creating-a-job" id="creating-a-job">Creating a Job</a></h1>
<p><code>listmonk</code> requires us to do the DB migrations before the app can run. The DB migrations are responsible for loading the schema in the Postgres database. Since this is a <em>one-off</em> task, Kubernetes provides a way to run such tasks by creating a <code>Job</code> object.</p>
<p>Since at the time of writing this guide <code>kubekutr</code> doesn't have support for <code>Jobs</code>/<code>CronJobs</code>, we can create a raw resource ourselves.</p>
<pre><code class="language-yml"># vim base/add-db-migration.yml
apiVersion: batch/v1
kind: Job
metadata:
  name: db-init
spec:
  template:
    spec:
      containers:
      - name: listmonk-db-init
        image: localhost:32000/listmonk:0.5
        command: [sh, -c, &quot;yes | ./listmonk --install&quot;]
        envFrom:
          - secretRef:
              name: app-secrets
      restartPolicy: Never
  backoffLimit: 5
  ttlSecondsAfterFinished: 10
  activeDeadlineSeconds: 100
</code></pre>
<p>When the Job object is applied to the cluster, a <code>Pod</code> is created. Just like how <code>Deployment</code> targets the Pod with ReplicaSet, <code>Job</code> manages the state of the <code>Pod</code>. Retry for a job or parallel scheduling can all be controlled with the <code>Job</code> spec about which you can read more <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">here</a>.</p>
<p>Every time we add a resource, we must remember to add it to our inventory, which is defined in <code>kustomization.yml</code>.</p>
<pre><code class="language-yml"># vim base/kustomization.yml
resources:
...
- add-db-migration.yml
...
</code></pre>
<p>Since this resource is now added to our inventory, all our <em>customisations</em> will be applied to the <code>Job</code> object as well.</p>
<blockquote>
<p><strong>Note</strong>: Always remember to add resources to <code>kustomization.yml</code> file to keep your inventory up to date. Not doing so, would mean that those resources would be skipped out when building the manifests.</p>
</blockquote>
<h1><a class="header" href="#adding-storage-to-db" id="adding-storage-to-db">Adding storage to DB</a></h1>
<p>Since <code>listmonk</code> relies on Postgres as it's storage backend to store email campaigns data, subscriber details, etc we need to attach a Persistent Volume to the pod running the DB. Many cloud providers have plugins baked in the storage provisioner (mentioned via StorageClass) but since we are doing a local deployment, we will make use of <code>hostpath provisioner</code> to create storage backed by the node on which the pod is running:</p>
<pre><code class="language-yml"># vim base/add-db-volume.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-storage
  labels:
    app.kubernetes.io/component: storage
spec:
  storageClassName: microk8s-hostpath
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &quot;/mnt/data&quot;
</code></pre>
<p>And like always, let's add this to the inventory:</p>
<pre><code class="language-yml"># vim base/kustomization.yml
resources:
- add-db-volume.yml
</code></pre>
<h1><a class="header" href="#deploying-on-microk8s" id="deploying-on-microk8s">Deploying on microk8s</a></h1>
<p>Now that the preflight changes have been done, let's take a look at how we will deploy to <code>microK8s</code>.</p>
<p>Kubernetes has an API server deployed in its control plane. This API server is responsible for making changes in the cluster. The API server is like the gateway to your cluster and all the updates can be managed with this. <code>kubectl</code> is a nifty wrapper for making HTTP calls to this API. In this section, we'll see how we declaratively <code>apply</code> changes to our cluster to get our app running.</p>
<p><img src="./img/k8s-api.png" alt="K8s API server" /></p>
<h2><a class="header" href="#step-1-create-a-namespace" id="step-1-create-a-namespace">Step 1: Create a namespace</a></h2>
<p>We need to create a namespace object. <code>Namespace</code> refers to a virtual cluster in Kubernetes. Since Kubernetes is a multi-tenant platform where multiple workloads are deployed, we need a layer of separation. Namespace acts as a virtual separation but remembers it does <strong>not</strong> mean isolation in terms of scheduling. You can set resource limits to a namespace, manage all the resources of an app collectively with one namespace, set network policies on the namespace, etc, set proper RBAC for teams to access a particular Namespace and restrict their actions as well.</p>
<blockquote>
<p><strong>Note</strong>: If you don't specify any namespace, <code>default</code> namespace is used. In production clusters, this is a <em>very</em> bad idea as you don't want to mix arbitrary workloads under one namespace.</p>
</blockquote>
<pre><code class="language-yml"># vim overlays/local/namespace.yml
apiVersion: v1
kind: Namespace
metadata:
  name: listmonk-local
</code></pre>
<pre><code class="language-shell"># cwd listmonk-base-deployment
microk8s.kubectl apply -f overlays/local/namespace.yml
</code></pre>
<p>A namespace should have been created with the name <strong>listmonk-local</strong>. You can verify the same if it's created by:</p>
<pre><code class="language-shell">microk8s.kubectl get namespace
</code></pre>
<h3><a class="header" href="#step-2-build-and-apply-manifest" id="step-2-build-and-apply-manifest">Step 2: Build and apply manifest</a></h3>
<pre><code class="language-shell">kustomize build overlays/local | microk8s.kubectl apply -f -
</code></pre>
<p>The resources are now applied in the cluster. You can see the status of pods by:</p>
<pre><code class="language-shell">$ microk8s.kubectl get pods -n listmonk-local
NAME                       READY   STATUS    RESTARTS   AGE
api-dev-5954dbc8fb-2242w   1/1     Running   0          4d
api-dev-5954dbc8fb-99vwx   1/1     Running   0          4d
api-dev-5954dbc8fb-nbm7j   1/1     Running   0          4d
</code></pre>
<h3><a class="header" href="#step-3-verifying-the-changes" id="step-3-verifying-the-changes">Step 3: Verifying the changes</a></h3>
<h4><a class="header" href="#get-the-pod-status" id="get-the-pod-status">Get the pod status</a></h4>
<pre><code class="language-shell">microk8s.kubectl get pods -n listmonk-local
</code></pre>
<h4><a class="header" href="#get-deployment-logs" id="get-deployment-logs">Get deployment logs</a></h4>
<pre><code class="language-shell">microk8s.kubectl logs deployment/listmonk-app-local -n listmonk-local

2020/03/28 09:36:12 main.go:89: reading config: /etc/listmonk/config.toml
2020/03/28 09:36:13 init.go:209: loaded SMTP: my0 (xxxxx@my.smtp.server)
2020/03/28 09:36:13 init.go:197: skipped SMTP: postal
⇨ http server started on [::]:9000
</code></pre>
<p>Yay! We can see <code>listmonk</code> is running successfully in the logs.</p>
<h4><a class="header" href="#get-service-details" id="get-service-details">Get Service details</a></h4>
<pre><code class="language-shell">microk8s.kubectl get service -n listmonk-local
</code></pre>
<blockquote>
<p><strong>Note</strong>: It's important to use <code>--namespace/-n</code> flag with every <code>kubectl</code> command <strong>if</strong> you don't set the namespace in your <code>current-context</code>. By default, the <code>default</code> namespace is set in the <code>current-context</code>, so you can set it to <code>listmonk-local</code> to avoid passing the flag <code>-n</code> for your convenience too.</p>
</blockquote>
<h3><a class="header" href="#further-steps" id="further-steps">Further Steps</a></h3>
<p>You can explore other <code>kubectl</code> commands <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">here</a>.</p>
<p>Head over to read the <a href="./06_00_Final_word.html">final note</a>.</p>
<h1><a class="header" href="#a-final-word" id="a-final-word">A final word</a></h1>
<p>Now that our application is running in Kubernetes successfully, we can go on to do better things in life. Like make a nice coffee, water the plants or send emails with <em>Listmonk</em> :).</p>
<p>Check out the <a href="./06_01_Final_additional.html">Additional Resources</a> section to find more resources that will help you with your Kubernetes journey.</p>
<p>This guide will be continuously updated to add more resources and tutorials to help make migration to Kubernetes seamless. If you're interested in contributing, head over to <a href="https://github.com/mr-karan/k8s-deployment-book">mr-karan/k8s-deployment-book</a> and propose changes by sending PRs/Issues.</p>
<p>You can checkout the source code for all the manifests used in this guide <a href="https://github.com/mr-karan/listmonk-infra">here</a>.</p>
<p>Thanks for reading till here.</p>
<h1><a class="header" href="#additional-resources" id="additional-resources">Additional Resources</a></h1>
<ul>
<li><a href="https://www.cncf.io/the-childrens-illustrated-guide-to-kubernetes/">The Illustrated Children’s Guide to Kubernetes</a></li>
<li><a href="https://www.cncf.io/phippy-goes-to-the-zoo-book/">Phippy Goes to the Zoo</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/">Kubernetes basics</a></li>
<li><a href="https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16">Kubernetes 101: Pods, Nodes, Containers, and Clusters</a></li>
</ul>
<blockquote>
<p>If you think I missed out on a <strong>great</strong> resource that <strong>deserves</strong> to be present here, feel free to send a PR for the same <a href="https://github.com/mr-karan/k8s-deployment-book">here</a>.</p>
</blockquote>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
